# extract_population_split.py (V14 – FINAL, KOMPATIBEL)
#
# Verbesserungen:
# - Erweiterte SHORT_POP_KEEP für Meta-Aussagen (SMOFKABIVEN, VABYSMO)
# - Neuer Fallback: fallback_population_from_summary() für L'essentiel (GIVLAARI)
# - Verbesserte Priorisierungs-Logik für Kandidaten
# - Konsistente MIN_POP_CHARS = 40
# - Subpopulation-Längen-Validierung
# - FIXES: Regex-Stop-Markers, alte CLI-Signatur, keine Duplikate

import sys
import csv
import re
import logging
from pathlib import Path
from typing import Dict, Tuple

from shared_utils import (
    load_patterns,
    normalize_text,
    read_pdf_text,
    split_by_headings_v3,
    find_section_with_rule,
    fallback_extract,
    iter_pdfs,
    make_doc_id,
    first_match_rule,
    start_signal,
    progress_bar,
    end_signal,
)

# Logging einrichten
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

PATTERN_VERSION = "hybrid_v2.1"
MIN_POP_CHARS = 40  # Konsistent auf 40 gesetzt
MIN_SUBPOP_CHARS = 20
MAX_SUBPOP_LENGTH = 800

# In extract_population_split.py

# STAGE 1: Negative Patterns (rausfiltern VOR Extraktion!)
POPULATION_BLACKLIST_PATTERNS = [
    r"^Demande\s+de\s+donn[ée]es",
    r"^La\s+Commission\s+souhaite",
    r"^\d+\s+CONCLUSIONS?\s+DE\s+LA\s+COMMISSION",
    r"^[ÉE]VALUER\s+LES\s+TECHNOLOGIES",
    r"^Recommandations?\s+de\s+la\s+Commission",
    r"^de\s+la\s+d[ée]monstration.*dans\s+la\s+population",  # SMR-Text
]

def is_blacklisted_population(text: str) -> bool:
    """Prüfe ob Text aus falscher Section stammt"""
    head = text.strip()[:200].lower()
    for pat in POPULATION_BLACKLIST_PATTERNS:
        if re.search(pat, head, re.I):
            return True
    return False

def validate_population_length(text: str) -> Tuple[str, str]:
    text = (text or "").strip()
    if not text:
        return "", "empty"

    if len(text) < 20:
        return "", "too_short_hard"

    # Kurze Meta-Aussagen behalten
    if len(text) < MIN_POP_CHARS:
        if is_short_but_valid_population(text):
            return text, "short_but_valid"
        # NICHT mehr verwerfen, sondern flaggen
        return text, "short_but_suspect"

    if is_blacklisted_population(text):
        return "", "blacklisted_section"

    if looks_numeric_only(text):
        # statt Verwerfen: behalten, aber flaggen
        return text, "numericish"

    return text, "ok"

# Erweiterte Keep-Patterns für kurze aber wichtige Population-Aussagen
SHORT_POP_KEEP = [
    r"population cible ne peut pas être estimée",
    r"population cible de cette sp[ée]cialit[ée]\s+ne peut\s+[êe]tre\s+estim[ée]e?",
    r"population cible\s+prévalente",
    r"population[s]?\s+cible[s]?\s+d[ée]j[àa]\s+estim[ée]e?s?",
    r"n['']est pas de nature [àa] modifier les populations cibles",
    r"population[s]?\s+cible[s]?\s+telle[s]?\s+que\s+d[ée]finie[s]?",
]

# Patterns für L'essentiel / Summary-Box
ESS_SUMMARY_PATTERNS = [
    r"avis favorable au remboursement[^.]*patients[^.]*\.",
    r"avis favorable[^.]*chez les patients[^.]*\.",
    r"avis\s+favorable[^.]{10,300}patients[^.]{10,300}\.",
]

# Subpopulation Stop-Marker (als Regex)
SUBPOP_STOP_MARKERS = [
    r"(?i)\bcomparateurs?\b",
    r"(?i)\bsynthèse\b",
    r"(?i)\bconclusion\b",
    r"(?i)\bservice médical rendu\b",
    r"(?i)\bSMR\b",
]


def _safe_section_and_rule(result) -> Tuple[str, str]:
    """Normalisiert die Rückgabe von find_section_with_rule."""
    if result is None:
        return "", ""
    if isinstance(result, tuple) and len(result) == 2:
        sec, rule_id = result
        return sec or "", rule_id or ""
    if isinstance(result, str):
        return result, ""
    return str(result), ""


def is_short_but_valid_population(text: str) -> bool:
    """Prüft ob ein kurzer Text trotzdem als gültige Population zählt."""
    if not text or len(text) < 20:
        return False
    
    text_lower = text.lower()
    for pat in SHORT_POP_KEEP:
        if re.search(pat, text_lower, flags=re.I):
            return True
    
    return False


def trim_population_block(text: str) -> str:
    """Entfernt Epidemiologie-Sections und kürzt den Population-Block sinnvoll."""
    if not text:
        return ""
    
    # Entferne Epidemiologie-Sections
    epidemio_patterns = [
        r"(?i)^\s*(?:\d+\.?\d*\s+)?[ÉE]PID[ÉE]MIOLOGIE\b.*?(?=\n\n|\Z)",
        r"(?i)\b[ÉE]PID[ÉE]MIOLOGIE\b[^\n]{0,100}\n.*?(?=\n\n|\Z)",
    ]
    
    for pat in epidemio_patterns:
        text = re.sub(pat, "", text, flags=re.DOTALL | re.MULTILINE)
    
    # Whitespace normalisieren
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    return text.strip()


def _normalize_apostrophes(s: str) -> str:
    """Normalisiert verschiedene Apostroph-Varianten aus PDFs."""
    return s.replace("'", "'").replace("`", "'").replace("'", "'")

def fallback_population_from_indication(ind_text: str) -> str:
    """Fallback: Extrahiert Population-Infos aus Indication (robust für Akzente)."""
    if not ind_text or len(ind_text) < 50:
        return ""
    
    # Apostroph-Normalisierung + Whitespace
    text = " ".join(ind_text.split())
    text = _normalize_apostrophes(text)  # Nur EINMAL!
    
    patterns = [
        r"chez les (?:adultes?|patients?|enfants?)[^.]{10,300}\.",
        r"(?:adultes?|patients?|enfants?)[^.]{10,250}[aâ]g[ée]s?\s+de[^.]{10,200}\.",
        r"patients?[^.]{10,250}atteints?\s+de[^.]{10,250}\.",
    ]
    
    for pat in patterns:
        m = re.search(pat, text, flags=re.I)
        if m:
            return m.group(0).strip()
    
    return ""

def fallback_population_from_summary(full_text: str) -> str:
    """
    Sucht in 'L'essentiel' / 'Synthèse de l'avis' nach einem
    Satz mit 'patients ...' als Population-Ersatz.
    """
    if not full_text:
        return ""
    
    t = " ".join(full_text.split())
    low = t.lower()
    
    # FIX: Duplikate entfernt
    window = ""
    for anchor in ["l'essentiel", "lessentiel", "synthèse de l'avis", "synthese de l'avis"]:
        idx = low.find(anchor)
        if idx != -1:
            window = t[idx : idx + 800]
            break
    
    if not window:
        window = t
    
    # Satz mit 'avis favorable ... patients ...'
    for pat in ESS_SUMMARY_PATTERNS:
        m = re.search(pat, window, flags=re.I)
        if m:
            result = m.group(0).strip()
            if len(result) >= MIN_POP_CHARS:
                return result
    
    # Generischer Satz: 'chez les patients ...'
    m = re.search(r"chez les patients[^.]{10,300}\.", window, flags=re.I)
    if m:
        result = m.group(0).strip()
        if len(result) >= MIN_POP_CHARS:
            return result
    
    return ""


def infer_sub_from_population(pop_text: str) -> str:
    """Heuristik: Versucht Subpopulation aus Population zu extrahieren."""
    if not pop_text or len(pop_text) < MIN_SUBPOP_CHARS:
        return ""
    
    sub_markers = [
        r"sous[-\s]?population[s]?",
        r"sous[-\s]?groupe[s]?",
        r"analyses?\s+en\s+sous[-\s]?groupe",
        r"population[s]?\s+sp[ée]cifique[s]?",
        r"patients?\s+en\s+[ée]chec",
        r"formes?\s+s[ée]v[èe]res?",
    ]
    
    for marker in sub_markers:
        if re.search(marker, pop_text, flags=re.I):
            match = re.search(marker, pop_text, flags=re.I)
            if match:
                start = max(0, match.start() - 100)
                end = min(len(pop_text), match.end() + 200)
                snippet = pop_text[start:end].strip()
                
                # Satzbasiertes Filtern
                sentences = re.split(r'(?<=[.!?])\s+', snippet)
                relevant = [
                    s for s in sentences
                    if re.search(
                        r"(sous[-\s]?population|sous[-\s]?groupe|formes?\s+s[ée]v[èe]res?|patients?\s+en\s+[ée]chec)",
                        s, flags=re.I
                    )
                ]
                
                if relevant:
                    clean_snippet = " ".join(relevant).strip()
                    if len(clean_snippet) >= MIN_SUBPOP_CHARS:
                        return clean_snippet
                
                # Fallback: Original wenn nichts relevantes
                if len(snippet) >= MIN_SUBPOP_CHARS:
                    return snippet
    
    return ""


def validate_and_trim_subpopulation(sub: str, pop: str) -> str:
    """
    Validiert und trimmt Subpopulation-Text.
    
    FIX: Stop-Marker jetzt mit Regex statt .find()
    """
    if not sub:
        return ""
    
    # 1) Stoppe an falschen Sections (FIX: mit Regex)
    for marker in SUBPOP_STOP_MARKERS:
        m = re.search(marker, sub, flags=re.I)
        if m and m.start() > 20:  # Nicht wenn Marker ganz am Anfang
            sub = sub[:m.start()].strip()
            break
    
    # 2) Längen-Limit
    max_len = min(MAX_SUBPOP_LENGTH, int(len(pop) * 1.5)) if pop else MAX_SUBPOP_LENGTH
    
    if len(sub) > max_len:
        logger.debug(f"Subpopulation too long ({len(sub)}), trimming to {max_len}")
        sub = sub[:max_len].strip()
        last_period = sub.rfind('.')
        if last_period > max_len * 0.6:
            sub = sub[:last_period + 1]
    
    # 3) Validierung: Verwerfe wenn offensichtlich falsch
    if len(sub) > len(pop) * 2.0:
        logger.warning(f"Subpopulation suspiciously long ({len(sub)} vs {len(pop)}), discarding")
        return ""
    
    # 4) Prüfe auf falsche Section-Marker im Text
    for marker in SUBPOP_STOP_MARKERS:
        if re.search(marker, sub, flags=re.I):
            logger.warning(f"Subpopulation contains stop marker, discarding")
            return ""
    
    return sub


def select_best_population_candidate(candidates: list) -> Tuple[str, str]:
    """Wählt den besten Population-Kandidaten aus mehreren Optionen."""
    if not candidates:
        return "", ""
    
    # Filter: mindestens MIN_POP_CHARS oder valid short text
    valid = []
    for text, length, source, rule in candidates:
        if length >= MIN_POP_CHARS or is_short_but_valid_population(text):
            valid.append((text, length, source, rule))
    
    if not valid:
        return "", ""
    
    # Sortiere: Summary > Section > Indication, dann nach Länge
    def sort_key(item):
        text, length, source, rule = item
        priority = {'summary': 3, 'section': 2, 'indication': 1}.get(source, 0)
        return (-priority, -length)
    
    valid.sort(key=sort_key)
    
    best_text, best_len, best_source, best_rule = valid[0]
    logger.debug(f"Selected population from {best_source} (length={best_len})")
    
    return best_text, best_rule

def clean_population_block_v2(text: str) -> Tuple[str, str]:
    """Trennt klinische Population von Epidemiologie"""
    if not text:
        return "", ""
    
    sentences = re.split(r'(?<=[.!?])\s+', text)
    epidemio_markers = [
        r"(?i)^Selon\s+(?:l['']Agence|les\s+données)",
        r"(?i)^La\s+pr[ée]valence",
        r"(?i)\best\s+estim[ée]e?\s+[àa].*\d+",
    ]
    
    clean_sentences = []
    for sent in sentences:
        if any(re.search(pat, sent) for pat in epidemio_markers):
            break
        if re.search(r"(?i)\b(?:patients?|adultes?|enfants?|atteints?)\b", sent):
            clean_sentences.append(sent)
    
    return ' '.join(clean_sentences).strip(), ""

def looks_numeric_only(text: str) -> bool:
    """Erkennt reine Epidemiologie/Zahlenblöcke, nicht klinische Populationen"""
    if not text:
        return True
    
    t = text.strip()
    if len(t) < 20:
        return True
    
    # Prüfe auf klinische Schlüsselwörter
    has_clinical = re.search(
        r"(?i)\b(patients?|atteints?|adultes?|enfants?|personnes|traitement|maladie)\b", 
        t
    ) is not None
    
    # Zähle Zahlen
    digit_count = len(re.findall(r'\d+', t))
    
    # Nur WIRKLICH zahlenlastige Texte ohne klinische Begriffe verwerfen
    if digit_count >= 5 and len(t) < 250 and not has_clinical:
        logger.debug(f"Rejected as numeric-only: {t[:80]}...")
        return True
    
    return False

def extract_one(pdf: Path, cfg: dict, ind_map: Dict[str, str]) -> Tuple[str, str, str, str]:
    """Extrahiert Population + Subpopulation aus einem PDF."""
    options = cfg.get("options", {})
    pop_target = cfg.get("targets", {}).get("Population", {})
    pop_pats = pop_target.get("patterns", [])
    pop_min_chars = int(pop_target.get("min_chars", MIN_POP_CHARS))
    
    raw = read_pdf_text(pdf)
    text = normalize_text(raw)
    blocks = split_by_headings_v3(text, options)
    doc_id = make_doc_id(str(pdf))
    
    # 1) Standard Population-Section
    pop, pop_rule = _safe_section_and_rule(
        find_section_with_rule(blocks, pop_pats, min_chars=pop_min_chars)
    )
    
    if not pop:
        pop_fb = fallback_extract(text, pop_pats, window=1000)
        pop = pop_fb or ""
        if pop:
            pop_rule = first_match_rule(text, pop_pats) or "fallback_patterns"
    
    # ✅ FIX 1: Korrekte Einrückung!
    pop = trim_population_block(pop or "")
    if pop:
        pop, pop_status = validate_population_length(pop)
        if not pop:
            logger.debug(f"{pdf.name}: Population rejected: {pop_status}")
            pop_rule = pop_status
    
    # ✅ NEU: Regel D - Epidemiologie abtrennen
    if pop:
        pop, _ = clean_population_block_v2(pop)
    
    # ✅ NEU: Regel C - Numerik-Check + Fallback auf Indikation
    if not pop or looks_numeric_only(pop):
        ind = ind_map.get(doc_id, "")
        
        # Fallback 1: aus Indication
        fb_ind = fallback_population_from_indication(ind)
        
        # Fallback 2: aus Summary
        fb_summary = fallback_population_from_summary(text)
        
        # Wähle besten Kandidaten
        candidates = [
            (pop, len(pop or ""), 'section', pop_rule),
            (fb_summary, len(fb_summary or ""), 'summary', 'from_summary'),
            (fb_ind, len(fb_ind or ""), 'indication', 'from_indication'),
        ]
        
        pop, pop_rule = select_best_population_candidate(candidates)
    
    # 5) Subpopulation - zweistufige Extraktion
    sub = ""
    sub_rule = ""

    if pop and len(pop) >= MIN_POP_CHARS:
        # STUFE 1: Versuche explizite Subpopulation-Section aus YAML
        sub_target = cfg.get("targets", {}).get("Subpopulation", {})
        sub_pats = sub_target.get("patterns", [])

        if sub_pats:
            sub_sec, sub_r = _safe_section_and_rule(
                find_section_with_rule(blocks, sub_pats, min_chars=20)
            )
            if sub_sec:
                cleaned = validate_and_trim_subpopulation(sub_sec, pop)
                if cleaned:
                    sub = cleaned
                    sub_rule = sub_r or "subpopulation_section"

        # STUFE 2: Fallback auf Heuristik aus Population
        if not sub:
            sub_candidate = infer_sub_from_population(pop)
            if sub_candidate:
                # Duplikate-Check
                if sub_candidate.strip() == pop.strip():
                    logger.debug(
                        "%s: Subpopulation identical to population, discarding",
                        pdf.name,
                    )
                else:
                    cleaned = validate_and_trim_subpopulation(sub_candidate, pop)
                    if cleaned:
                        sub = cleaned
                        # nur setzen, wenn wir nicht schon eine Section-Regel haben
                        if not sub_rule:
                            sub_rule = "inferred"
  
def main() -> None:
    if len(sys.argv) < 5:
        print("Usage: python extract_population_split.py <pdf_or_dir> <out_csv> <patterns_yaml> <indication_csv>")
        sys.exit(1)
    
    src = Path(sys.argv[1])
    out = Path(sys.argv[2])
    yaml_cfg = Path(sys.argv[3])
    ind_csv_path = Path(sys.argv[4])
    
    # Lade Indication-Map
    ind_map = {}
    if ind_csv_path.exists():
        with ind_csv_path.open("r", encoding="utf-8") as f:
            reader = csv.DictReader(f, delimiter=";")
            for row in reader:
                ind_map[row.get("doc_id", "")] = row.get("text", "")
        logger.info(f"Loaded {len(ind_map)} indications for fallback")
    
    # YAML einmal laden (Performance-Fix)
    cfg = load_patterns(yaml_cfg)
    logger.info("YAML patterns loaded once")
    
    pdfs = list(iter_pdfs(src))
    start_signal("extract_population_split", len(pdfs))
    
    rows = []
    
    for idx, pdf in enumerate(pdfs, start=1):
        try:
            # ✅ KORREKT: cfg und ind_map übergeben, 4 Werte empfangen
            pop_txt, sub_txt, pop_rule, sub_rule = extract_one(pdf, cfg, ind_map)
            
            doc_id = make_doc_id(str(pdf))
            
            rows.append([
                str(pdf),
                doc_id,
                PATTERN_VERSION,
                pop_txt or "",
                sub_txt or "",
                pop_rule or "",
                sub_rule or "",
            ])
            
        except Exception as e:
            logger.error(f"Error processing {pdf.name}: {type(e).__name__} - {str(e)}", exc_info=True)
            doc_id = make_doc_id(str(pdf))
            
            rows.append([
                str(pdf),
                doc_id,
                PATTERN_VERSION,
                "",
                "",
                f"__ERROR__:{type(e).__name__}",
                "",
            ])
        
        progress_bar(idx, len(pdfs), prefix="Population ")

    print()
    end_signal("extract_population_split")
    
    # Schreibe eine CSV im alten Format
    out.parent.mkdir(parents=True, exist_ok=True)
    with out.open("w", newline="", encoding="utf-8") as f:
        w = csv.writer(f, delimiter=";")
        w.writerow(["file", "doc_id", "pattern_version", "Population", "Subpopulation", "rule_id_population", "rule_id_subpopulation"])
        w.writerows(rows)
    
    logger.info(f"Processed {len(pdfs)} PDFs, saved to {out}")

if __name__ == "__main__":
    main()
